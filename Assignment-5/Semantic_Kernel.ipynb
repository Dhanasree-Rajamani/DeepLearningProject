{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7n269tone6j",
        "outputId": "7e372156-c64d-4a5d-a738-94c345c64fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting semantic-kernel==0.2.3.dev0\n",
            "  Downloading semantic_kernel-0.2.3.dev0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.24.2\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asyncio<4.0.0,>=3.4.3\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0.0,>=23.1.0\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting openai<0.28.0,>=0.27.0\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (1.26.15)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.3.dev0) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: asyncio, numpy, multidict, frozenlist, async-timeout, aiofiles, yarl, aiosignal, aiohttp, openai, semantic-kernel\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 asyncio-3.4.3 frozenlist-1.3.3 multidict-6.0.4 numpy-1.24.2 openai-0.27.4 semantic-kernel-0.2.3.dev0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install semantic-kernel==0.2.3.dev0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import semantic_kernel as sk\n",
        "\n",
        "kernel = sk.Kernel()"
      ],
      "metadata": {
        "id": "tdruxEgLnmhT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Set OPEN-AI key"
      ],
      "metadata": {
        "id": "r59aitsLoNXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from semantic_kernel.ai.open_ai import OpenAITextCompletion\n",
        "\n",
        "api_key = \"\"\n",
        "\n",
        "kernel.config.add_text_backend(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ff3dwGn-2i",
        "outputId": "d106da86-16cc-49b8-ec2f-3815f35c3e43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<semantic_kernel.kernel_config.KernelConfig at 0x7fd1fc727550>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel.config.add_text_backend(               # We are adding a text backend\n",
        "    \"OpenAI_davinci\",                         # The alias we can use in prompt templates' config.json\n",
        "    OpenAITextCompletion(\n",
        "        \"text-davinci-003\",                   # OpenAI Model Name\n",
        "        \"\"          # OpenAI API key\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkQxFbP8n_jF",
        "outputId": "cfa33ad9-0cbf-4c65-ee67-b9ce5c88b97e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<semantic_kernel.kernel_config.KernelConfig at 0x7fd1fc727550>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"{{$input}}\n",
        "Translate the content above into telugu and tamil.\n",
        "\"\"\"\n",
        "\n",
        "summarize = kernel.create_semantic_function(prompt, max_tokens=2000, temperature=0.2, top_p=0.5)"
      ],
      "metadata": {
        "id": "DlYN2WkAocBe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "Hi, This is Sravani. I have two friends. They are Dhanasree and Devi Priya.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IZsRu_gvp_S8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(input_text)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH3qTfqNqDS6",
        "outputId": "d02d0a2a-0e1f-49c2-cb3c-b90b5fe199a7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Telugu: నేను రెండు మిత్రులు ఉన్నాను. అవి ధనస్రీ మరియు దేవీ ప్రియా అనేది.\n",
            "\n",
            "Tamil: எனக்கு இரு நண்பர்கள் உள்ளன. அவர்கள் தனச்சிரி மற்றும் தேவி ப்ரியா ஆகும்.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk_prompt = \"\"\"\n",
        "{{$input}}\n",
        "\n",
        "Give me the TLDR in 5 words.\n",
        "\"\"\"\n",
        "\n",
        "text = \"\"\"\n",
        "    1) A robot may not injure a human being or, through inaction,\n",
        "    allow a human being to come to harm.\n",
        "\n",
        "    2) A robot must obey orders given it by human beings except where\n",
        "    such orders would conflict with the First Law.\n",
        "\n",
        "    3) A robot must protect its own existence as long as such protection\n",
        "    does not conflict with the First or Second Law.\n",
        "\"\"\"\n",
        "\n",
        "tldr_function = kernel.create_semantic_function(sk_prompt, max_tokens=200, temperature=0, top_p=0.5)\n",
        "\n",
        "summary = tldr_function(text)\n",
        "\n",
        "print(f\"Output: {summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWS7XOQJqlJm",
        "outputId": "4c50a3a5-8dfd-4b03-8cca-31f84efcde64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: \n",
            "Robots must not harm humans.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "MDv8r01Bsdnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sk_prompt = \"\"\"\n",
        "ChatBot can have a conversation with you about any topic.\n",
        "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
        "\n",
        "{{$history}}\n",
        "User: {{$user_input}}\n",
        "ChatBot: \"\"\""
      ],
      "metadata": {
        "id": "QtoH-ClVqoW0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_function = kernel.create_semantic_function(sk_prompt, \"ChatBot\", max_tokens=2000, temperature=0.7, top_p=0.5)"
      ],
      "metadata": {
        "id": "mSHARLOxsh4X"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = sk.ContextVariables()\n",
        "context[\"history\"] = \"\""
      ],
      "metadata": {
        "id": "317b9qrcsk6o"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context[\"user_input\"] = \"Hi, I'm looking for book suggestions\"\n",
        "bot_answer = await chat_function.invoke_with_vars_async(input=context)\n",
        "print(bot_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJozv1SFsp_5",
        "outputId": "a1587e41-2c05-41cb-8f21-aafe8ebf209c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hi there! What kind of books are you looking for?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context[\"history\"] += f\"\\nUser: {context['user_input']}\\nChatBot: {bot_answer}\\n\"\n",
        "print(context[\"history\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnMJ-eLGsr--",
        "outputId": "2ec1868d-0ba9-4d81-8fe2-d785a1b668e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Hi, I'm looking for book suggestions\n",
            "ChatBot:  Hi there! What kind of books are you looking for?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def chat(input_text: str) -> None:\n",
        "    # Save new message in the context variables\n",
        "    print(f\"User: {input_text}\")\n",
        "    context[\"user_input\"] = input_text\n",
        "\n",
        "    # Process the user message and get an answer\n",
        "    answer = await chat_function.invoke_with_vars_async(context)\n",
        "\n",
        "    # Show the response\n",
        "    print(f\"ChatBot: {answer}\")\n",
        "\n",
        "    # Append the new interaction to the chat history\n",
        "    context[\"history\"] += f\"\\nUser: {input_text}\\nChatBot: {answer}\\n\""
      ],
      "metadata": {
        "id": "GSCJzr4SsvY3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await chat(\"I love to listen some jokes. Can you crack some jokes?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIBni-M-szI9",
        "outputId": "a402a84c-a18a-4c91-af08-42854dab428a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I love to listen some jokes. Can you crack some jokes?\n",
            "ChatBot:  Sure! Why did the chicken cross the playground? To get to the other slide!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await chat(\"Some more jokes please\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt1VMAFYs1eF",
        "outputId": "86c8de30-f79b-4b0f-9d48-9826186050bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Some more jokes please\n",
            "ChatBot:  Why did the scarecrow win the Nobel Prize? Because he was outstanding in his field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await chat(\"Can you tell jokes in english. But some big jokes instead of one line jokes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdsRQJ2As5eD",
        "outputId": "13d36e40-a9cc-409d-cdd1-91d8865b2bd9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Can you tell jokes in english. But some big jokes instead of one line jokes\n",
            "ChatBot:  Sure! Here's one: A man walks into a bar and orders a drink. The bartender says, \"I'm sorry, sir, but you look a lot like my father.\" The man replies, \"That's okay, I'm looking for him too.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await chat(\"I didnt understand the joke. Can you explain it more\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QBdiR79tfZL",
        "outputId": "116f5fe2-c76b-4c28-946b-e28c0970b90d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I didnt understand the joke. Can you explain it more\n",
            "ChatBot:  Sure! The joke is about a man who walks into a bar and orders a drink. The bartender notices that the man looks a lot like his father, so he comments on it. The man replies that he's actually looking for his father, so the joke is that he's not only looking for his father, but he looks like him too.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await chat(\"I think its a bad joke. Give me some sensible joke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xacJqKkbtzIT",
        "outputId": "29664e87-d580-4a35-ec0b-7400c00f8df3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: I think its a bad joke. Give me some sensible joke\n",
            "ChatBot:  Sure! Here's one: What did the fish say when it hit the wall? Dam!\n"
          ]
        }
      ]
    }
  ]
}
