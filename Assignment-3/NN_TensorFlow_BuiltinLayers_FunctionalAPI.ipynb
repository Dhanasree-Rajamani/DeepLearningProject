{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3 layer Neural Network using Tensorflow builtin layers"
      ],
      "metadata": {
        "id": "joKoIkANTdoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras provides you with a wide range of built-in layers, so that you don't have to implement your own layers all the time.\n",
        "\n",
        "Convolution layers \\\n",
        "Transposed convolutions \\\n",
        "Separateable convolutions \\\n",
        "Average and max pooling \\\n",
        "Global average and max pooling \\\n",
        "LSTM, GRU (with built-in cuDNN acceleration) \\\n",
        "BatchNormalization \\\n",
        "Dropout \\\n",
        "Attention \\\n",
        "ConvLSTM2D \\\n",
        "etc."
      ],
      "metadata": {
        "id": "O-Un6p-kTzDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "EKVXQ4QDUgRu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32):\n",
        "      super(Linear, self).__init__()\n",
        "      self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "      self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "# Instantiate our lazy layer.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert len(linear_layer.weights) == 2"
      ],
      "metadata": {
        "id": "doY39uamUS8e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "metadata": {
        "id": "NsHtJwnyUmce"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights are automatically tracked under the `weights` property.\n",
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "metadata": {
        "id": "M4QfSNRxUntM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fvRxhzjXTRrY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Dropout(Layer):\n",
        "  \n",
        "  def __init__(self, rate):\n",
        "    super(Dropout, self).__init__()\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "\n",
        "class MLPWithDropout(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "      super(MLPWithDropout, self).__init__()\n",
        "      self.linear_1 = Linear(32)\n",
        "      self.dropout = Dropout(0.5)\n",
        "      self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "      x = self.linear_1(inputs)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.dropout(x, training=training)\n",
        "      return self.linear_3(x)\n",
        "    \n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((2, 2)), training=True)\n",
        "y_test = mlp(tf.ones((2, 2)), training=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
        "# This is the deep learning equivalent of *declaring a type*.\n",
        "# The shape argument is per-sample; it does not include the batch size.\n",
        "# The functional API focused on defining per-sample transformations.\n",
        "# The model we create will automatically batch the per-sample transformations,\n",
        "# so that it can be called on batches of data.\n",
        "inputs = tf.keras.Input(shape=(16,))\n",
        "\n",
        "# We call layers on these \"type\" objects\n",
        "# and they return updated types (new shapes/dtypes).\n",
        "x = Linear(32)(inputs) # We are reusing the Linear layer we defined earlier.\n",
        "x = Dropout(0.5)(x) # We are reusing the Dropout layer we defined earlier.\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# A functional `Model` can be defined by specifying inputs and outputs.\n",
        "# A model is itself a layer like any other.\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# A functional model already has weights, before being called on any data.\n",
        "# That's because we defined its input shape in advance (in `Input`).\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "# Let's call our model on some data.\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "metadata": {
        "id": "_2mNE3eiT8sR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "2VQU6zQPVNg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential([Linear(32), Dropout(0.5), Linear(10)])\n",
        "\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "metadata": {
        "id": "QlwRG6brUFzE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "OsjkMHGjVPfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "y_true = [0., 0., 1., 1.]  # Targets\n",
        "y_pred = [1., 1., 1., 0.]  # Predictions\n",
        "loss = bce(y_true, y_pred)\n",
        "print('Loss:', loss.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlXEaMbAVLGE",
        "outputId": "59cfe1ac-147b-4d50-a171-a2cc5553bdfc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 11.522857\n"
          ]
        }
      ]
    }
  ]
}