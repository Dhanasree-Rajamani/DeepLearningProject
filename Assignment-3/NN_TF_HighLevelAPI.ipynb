{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZXqYykhZVpKB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer"
      ],
      "metadata": {
        "id": "Lx4QBmhEWviO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, units=32, input_dim=32):\n",
        "      super(Linear, self).__init__()\n",
        "      w_init = tf.random_normal_initializer()\n",
        "      self.w = tf.Variable(\n",
        "          initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
        "          trainable=True)\n",
        "      b_init = tf.zeros_initializer()\n",
        "      self.b = tf.Variable(\n",
        "          initial_value=b_init(shape=(units,), dtype='float32'),\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "-7JMsBkSWrYe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate our layer.\n",
        "linear_layer = Linear(units=4, input_dim=2)\n",
        "\n",
        "\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "metadata": {
        "id": "TV3ENIy7WyV5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "metadata": {
        "id": "1w1abcvVXC_O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights"
      ],
      "metadata": {
        "id": "bx0omKbFXISS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "  def __init__(self, units=32):\n",
        "      super(Linear, self).__init__()\n",
        "      self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "      self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "# Instantiate our lazy layer.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# This will also call `build(input_shape)` and create the weights.\n",
        "y = linear_layer(tf.ones((2, 2)))"
      ],
      "metadata": {
        "id": "VQ1NeqdMXFjo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradients"
      ],
      "metadata": {
        "id": "meCYKg2UXNZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a dataset.\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Instantiate our linear layer (defined above) with 10 units.\n",
        "linear_layer = Linear(10)\n",
        "\n",
        "# Instantiate a logistic loss function that expects integer targets.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "# Iterate over the batches of the dataset.\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  \n",
        "  # Open a GradientTape.\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Forward pass.\n",
        "    logits = linear_layer(x)\n",
        "\n",
        "    # Loss value for this batch.\n",
        "    loss = loss_fn(y, logits)\n",
        "     \n",
        "  # Get gradients of weights wrt the loss.\n",
        "  gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
        "  \n",
        "  # Update the weights of our linear layer.\n",
        "  optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
        "  \n",
        "  # Logging.\n",
        "  if step % 100 == 0:\n",
        "    print('Step:', step, 'Loss:', float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTli3ooOXKwh",
        "outputId": "5e3477fe-7310-4e80-dd08-e94bdcdd6c52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Step: 0 Loss: 2.467670440673828\n",
            "Step: 100 Loss: 2.3106894493103027\n",
            "Step: 200 Loss: 2.2388761043548584\n",
            "Step: 300 Loss: 2.1261556148529053\n",
            "Step: 400 Loss: 2.016288995742798\n",
            "Step: 500 Loss: 1.9093151092529297\n",
            "Step: 600 Loss: 1.7734405994415283\n",
            "Step: 700 Loss: 1.8460146188735962\n",
            "Step: 800 Loss: 1.6618050336837769\n",
            "Step: 900 Loss: 1.5766392946243286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ComputeSum(keras.layers.Layer):\n",
        "  \"\"\"Returns the sum of the inputs.\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "      super(ComputeSum, self).__init__()\n",
        "      # Create a non-trainable weight.\n",
        "      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
        "                               trainable=False)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "      return self.total  \n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [2. 2.]\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [4. 4.]\n",
        "\n",
        "assert my_sum.weights == [my_sum.total]\n",
        "assert my_sum.non_trainable_weights == [my_sum.total]\n",
        "assert my_sum.trainable_weights == []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8qCKfaiXOsa",
        "outputId": "e834d2f0-e4b5-4d64-ac32-6ddc783d6743"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reuse the Linear class\n",
        "# with a `build` method that we defined above.\n",
        "\n",
        "class MLP(keras.layers.Layer):\n",
        "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "mlp = MLP()\n",
        "\n",
        "# The first call to the `mlp` object will create the weights.\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "\n",
        "# Weights are recursively tracked.\n",
        "assert len(mlp.weights) == 6"
      ],
      "metadata": {
        "id": "TzAZqg1sXUb-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = keras.Sequential([keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "                        keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "                        keras.layers.Dense(10)])"
      ],
      "metadata": {
        "id": "4YD9q-w9XWoO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Tracking"
      ],
      "metadata": {
        "id": "r8TuZEXdXb5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivityRegularization(keras.layers.Layer):\n",
        "  \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
        "  \n",
        "  def __init__(self, rate=1e-2):\n",
        "    super(ActivityRegularization, self).__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # We use `add_loss` to create a regularization loss\n",
        "    # that depends on the inputs.\n",
        "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "AkLD7RL4XYtU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the loss layer in a MLP block.\n",
        "\n",
        "class SparseMLP(keras.layers.Layer):\n",
        "  \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      super(SparseMLP, self).__init__()\n",
        "      self.linear_1 = Linear(32)\n",
        "      self.regularization = ActivityRegularization(1e-2)\n",
        "      self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      x = self.linear_1(inputs)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.regularization(x)\n",
        "      return self.linear_3(x)\n",
        "    \n",
        "\n",
        "mlp = SparseMLP()\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "\n",
        "print(mlp.losses)  # List containing one float32 scalar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvS3SEkaXdsm",
        "outputId": "5bd1a770-4a04-4cb0-9cda-ae3502e4437f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.28619078>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These losses are cleared by the top-level layer at the start of each forward pass -- they don't accumulate. layer.losses always contains only the losses created during the last forward pass. You would typically use these losses by summing them before computing your gradients when writing a training loop."
      ],
      "metadata": {
        "id": "bj0cKqN9XiLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Losses correspond to the *last* forward pass.\n",
        "mlp = SparseMLP()\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1  # No accumulation.\n",
        "\n",
        "# Let's demonstrate how to use these losses in a training loop.\n",
        "\n",
        "# Prepare a dataset.\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# A new MLP.\n",
        "mlp = SparseMLP()\n",
        "\n",
        "# Loss and optimizer.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Forward pass.\n",
        "    logits = mlp(x)\n",
        "\n",
        "    # External loss value for this batch.\n",
        "    loss = loss_fn(y, logits)\n",
        "    \n",
        "    # Add the losses created during the forward pass.\n",
        "    loss += sum(mlp.losses)\n",
        "     \n",
        "    # Get gradients of weights wrt the loss.\n",
        "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "  \n",
        "  # Update the weights of our linear layer.\n",
        "  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "  \n",
        "  # Logging.\n",
        "  if step % 100 == 0:\n",
        "    print('Step:', step, 'Loss:', float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IM4_yeyXf7H",
        "outputId": "413a5740-9d93-4597-81a1-e8f42e6e84f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0 Loss: 6.676252365112305\n",
            "Step: 100 Loss: 2.5746729373931885\n",
            "Step: 200 Loss: 2.4075963497161865\n",
            "Step: 300 Loss: 2.3879988193511963\n",
            "Step: 400 Loss: 2.3600149154663086\n",
            "Step: 500 Loss: 2.351372718811035\n",
            "Step: 600 Loss: 2.326296806335449\n",
            "Step: 700 Loss: 2.322831869125366\n",
            "Step: 800 Loss: 2.3203039169311523\n",
            "Step: 900 Loss: 2.325289011001587\n"
          ]
        }
      ]
    }
  ]
}